name: Sync Rules Files (Refactored)

on:
  push:
    paths:
      - rules-list.txt
  schedule:
    - cron: '0 4 * * *'
  workflow_dispatch:

env:
  RULES_LIST: rules-list.txt
  MAPPING_FILE: mapping.csv
  TEMP_MAPPING_FILE: temp_mapping.csv
  DEL_LIST_FILE: del_list.txt
  OUTPUT_DIR: rules

jobs:
  sync-rules:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Python Dependencies
        run: pip install requests
        working-directory: ${{ github.workspace }}

      - name: Create Output Directory
        run: mkdir -p ${{ env.OUTPUT_DIR }}
        working-directory: ${{ github.workspace }}

      - name: Process Rules List and Update Mapping
        id: process_rules
        working-directory: ${{ github.workspace }}
        run: |
          python3 << 'EOF'
          import csv
          import os
          import requests
          import hashlib
          from urllib.parse import urlparse
          from datetime import datetime

          rules_list_path = "${{ env.RULES_LIST }}"
          mapping_file_path = "${{ env.MAPPING_FILE }}"
          temp_mapping_file_path = "${{ env.TEMP_MAPPING_FILE }}"
          del_list_file_path = "${{ env.DEL_LIST_FILE }}"
          output_dir = "${{ env.OUTPUT_DIR }}"

          def calculate_hash(file_path):
              hash_md5 = hashlib.md5()
              with open(file_path, "rb") as f:
                  for chunk in iter(lambda: f.read(4096), b""):
                      hash_md5.update(chunk)
              return hash_md5.hexdigest()

          # ---  Mapping File Initialization ---
          if not os.path.isfile(mapping_file_path):
              with open(mapping_file_path, mode='w', newline='') as file:
                  writer = csv.writer(file)
                  writer.writerow(['URL', 'Remote_Filename', 'Local_Filename', 'Hash'])
          existing_mappings = {}
          if os.path.isfile(mapping_file_path):
              with open(mapping_file_path, mode='r', newline='') as file:
                  reader = csv.DictReader(file)
                  for row in reader:
                      existing_mappings[row['URL']] = row

          # ---  Process Rules List and Detect Changes ---
          new_mappings = {}
          with open(rules_list_path, mode='r') as file:
              for line in file:
                  url = line.strip()
                  if url.startswith(('http://', 'https://')):
                      try:
                          response = requests.head(url, allow_redirects=True, timeout=10) # 添加超时
                          response.raise_for_status() # 检查 HTTP 错误
                          remote_filename = os.path.basename(urlparse(response.url).path)
                          response = requests.get(url, stream=True, timeout=10) # 再次请求内容，并添加超时
                          response.raise_for_status() # 检查 HTTP 错误
                          remote_hash = hashlib.md5(response.content).hexdigest()
                          local_filename = remote_filename

                          existing_hash = existing_mappings.get(url, {}).get('Hash')
                          if url not in existing_mappings or existing_hash != remote_hash:
                              new_mappings[url] = {
                                  'Remote_Filename': remote_filename,
                                  'Local_Filename': local_filename,
                                  'Hash': remote_hash
                              }
                      except requests.exceptions.RequestException as e:
                          print(f"Error processing URL: {url} - {e}") # 打印错误信息，但 *不* 中断 Workflow
                          continue #  *继续处理*  rules-list.txt 中的 *下一个 URL*

          # ---  Handle Filename Conflicts ---
          filename_count = {}
          conflict_remote_filenames = set()
          existing_local_filenames = set()
          for row in existing_mappings.values():
              existing_local_filenames.add(row['Local_Filename'])

          for url, info in new_mappings.items():
              remote_filename = info['Remote_Filename']
              if remote_filename in filename_count:
                  filename_count[remote_filename] += 1
              else:
                  filename_count[remote_filename] = 1
              if filename_count[remote_filename] > 1 or remote_filename in existing_local_filenames:
                  conflict_remote_filenames.add(remote_filename)

          updated_mappings = {}
          for url, row in existing_mappings.items():
              local_filename = row['Local_Filename']
              if row['Remote_Filename'] in conflict_remote_filenames:  # 使用 Remote_Filename
                  parsed_url = urlparse(url)
                  last_segment = parsed_url.path.split('/')[-2] if len(parsed_url.path.split('/')) >= 3 and parsed_url.path.split('/')[-2] else 'default' # 避免索引错误
                  local_filename = f"{last_segment}-{row['Remote_Filename']}" # 使用 Remote_Filename
              updated_mappings[url] = {'Remote_Filename': row['Remote_Filename'], 'Local_Filename': local_filename, 'Hash': row['Hash']}

          for url, info in new_mappings.items():
              local_filename = info['Local_Filename']
              if info['Remote_Filename'] in conflict_remote_filenames: # 使用 Remote_Filename
                  parsed_url = urlparse(url)
                  last_segment = parsed_url.path.split('/')[-2] if len(parsed_url.path.split('/')) >= 3 and parsed_url.path.split('/')[-2] else 'default' # 避免索引错误
                  local_filename = f"{last_segment}-{info['Remote_Filename']}" # 使用 Remote_Filename
              updated_mappings[url] = {'Remote_Filename': info['Remote_Filename'], 'Local_Filename': local_filename, 'Hash': info['Hash']}


          # --- Update Temp Mapping File ---
          with open(temp_mapping_file_path, mode='w', newline='') as temp_file:
              writer = csv.writer(temp_file)
              writer.writerow(['URL', 'Remote_Filename', 'Local_Filename', 'Hash'])
              for url, info in updated_mappings.items():
                  writer.writerow([url, info['Remote_Filename'], info['Local_Filename'], info['Hash']])

          # --- Generate Delete List ---
          del_list = []
          updated_urls = set(updated_mappings.keys())
          for url, row in existing_mappings.items():
              if url not in updated_urls:
                  del_list.append(row['Local_Filename'])

          with open(del_list_file_path, mode='w') as del_file:
              for filename in del_list:
                  del_file.write(f"{filename}\n")

          print(f"Temp mapping file: {temp_mapping_file_path}")
          print(f"Working directory: {os.getcwd()}")
          os.replace(temp_mapping_file_path, mapping_file_path)

          EOF

      - name: Download Updated Files
        working-directory: ${{ github.workspace }}
        run: |
          if [ -f "${{ env.TEMP_MAPPING_FILE }}" ]; then
            while IFS=',' read -r url remote_filename local_filename hash; do
              local_file="${{ env.OUTPUT_DIR }}/${local_filename}"
              if ! test -f "$local_file" || [ "$(md5sum $local_file | awk '{ print $1 }')" != "$hash" ]; then
                echo "Downloading: $url to $local_file"
                wget --continue -O "$local_file" "$url" || echo "Failed to download $url. Skipping."
              fi
            done < <(tail -n +2 "${{ env.TEMP_MAPPING_FILE }}")
          else
            echo "No new or updated files to download."
          fi

      - name: Delete Obsolete Files
        working-directory: ${{ github.workspace }}
        run: |
          if [ -f "${{ env.DEL_LIST_FILE }}" ]; then
            while IFS= read -r filename; do
              echo "Deleting obsolete file: ${{ env.OUTPUT_DIR }}/$filename"
              rm -f "${{ env.OUTPUT_DIR }}/$filename" || echo "Failed to delete $filename. Skipping."
            done < "${{ env.DEL_LIST_FILE }}"
          else
            echo "No obsolete files to delete."
          fi

      - name: Update Mapping CSV and Cleanup
        working-directory: ${{ github.workspace }}
        run: |
          rm "${{ env.TEMP_MAPPING_FILE }}"
          rm "${{ env.DEL_LIST_FILE }}"
          echo "Mapping CSV updated and temporary files cleaned up."

      - name: List Files in Rules Directory (Post-Sync)
        working-directory: ${{ github.workspace }}
        run: ls -la "${{ env.OUTPUT_DIR }}"

      - name: Commit and Push Changes
        working-directory: ${{ github.workspace }}
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add "${{ env.OUTPUT_DIR }}"
          git add "${{ env.MAPPING_FILE }}"
          git diff-index --quiet HEAD || git commit -m "Update rules files from rules-list.txt"
          git push || echo "No changes to push." #  添加 push || echo 防止没有 commit 时 push 失败


**主要重构和改进说明:**

1.  **更清晰的 Step  划分和命名:**  将 Workflow  步骤划分得更细致，  每个 Step  专注于一个明确的任务，  并使用更具描述性的 Step  名称 (例如  "Checkout Repository",  "Process Rules List and Update Mapping",  "Download Updated Files"  等)，  提高了 Workflow  的 *可读性和可维护性*。

2.  **显式  `working-directory`  配置:**  在 *所有需要访问仓库文件的 Step*  中，  都 *显式地配置了  `working-directory: ${{ github.workspace }}`*，  确保所有相关操作都在 *Workflow  工作区根目录*  下执行，  避免路径问题。

3.  **Python  脚本内联，  但更注重代码组织:**  Python  脚本仍然采用内联  `python3 << 'EOF'`  的方式，  但脚本内部的代码 *增加了注释*，  将逻辑划分为更清晰的 *代码块*  (例如  "Mapping File Initialization",  "Process Rules List and Detect Changes",  "Handle Filename Conflicts"  等)，  提高了 Python  代码的 *可读性*。

4.  **更完善的错误处理 (Python 部分):**  在 Python  脚本的  "Process Rules List and Detect Changes"  代码块中，  为  `requests.head()`  和  `requests.get()`  请求 *添加了  `timeout=10`  参数*，  设置了 *请求超时时间*，  防止因网络问题导致 Workflow  长时间卡顿。  同时，  使用了  `try...except requests.exceptions.RequestException as e:`  块来 *捕获  `requests`  请求可能发生的异常* (例如网络错误，  HTTP 错误等)，  并在发生异常时 *打印错误信息*  (使用  `print(f"Error processing URL: {url} - {e}")`)，  但 *Workflow  会 *继续执行*，  不会因单个 URL  处理失败而中断*，  提高了 Workflow  的 *健壮性*。  使用了  `response.raise_for_status()`  来 *显式检查 HTTP  响应状态码*，  并在发生 HTTP 错误时抛出异常，  以便  `except`  块捕获并处理。

5.  **Download  和  Delete  Step  增加日志输出:**  在  "Download Updated Files"  和  "Delete Obsolete Files"  Step  的  `run`  脚本中，  为  `wget`  下载和  `rm`  删除命令 *增加了  `echo`  日志输出* (例如  `echo "Downloading: $url to $local_file"`，  `echo "Deleting obsolete file: ${{ env.OUTPUT_DIR }}/$filename"`），  使 Workflow  运行时，  日志信息更丰富，  可以 *更清晰地了解文件下载和删除的执行情况*。

6.  **Commit  和  Push  Step  优化:**  在  "Commit and Push Changes"  Step  中，  在  `git push`  命令后 *添加了  `|| echo "No changes to push."`*，  防止当没有 Commit  时，  `git push`  命令因 *没有本地 Commit  而失败*，  导致 Workflow  出错。  现在即使没有 Commit，  `git push`  命令也会 *输出 "No changes to push."  信息，  但 *不会导致 Step  失败*，  Workflow  可以 *顺利完成*。

7.  **移除  "Update Mapping CSV and Cleanup"  Step  中的  `cp`  命令:**  在  "Update Mapping CSV and Cleanup"  Step  中，  *移除了之前尝试使用的  `cp "${{ env.TEMP_MAPPING_FILE }}" "${{ env.MAPPING_FILE }}"`  命令*。  因为 Python  脚本中已经使用了  `os.replace(temp_mapping_file_path, mapping_file_path)`  来 *原子性地替换  mapping.csv  文件*，  已经完成了  `mapping.csv`  的更新，  因此  "Update Mapping CSV and Cleanup"  Step  现在 *只需要负责 *清理临时文件  (删除  `temp_mapping.csv`  和  `del_list.txt`)*  即可，  逻辑更清晰，  也避免了可能的重复操作。


**请您将您  `.github/workflows/sync-rules.yml`  文件内容 *替换为 *上面 *我提供的 *重构后的 YAML 代码*，  *仔细检查，  确保复制完整，  并且 YAML  文件的缩进和格式都是正确的*。  然后，  *重新上传您的  `.github/workflows/sync-rules.yml`  文件，  并 *再次手动运行您的 Workflow***。**

这次重构旨在 *提高 Workflow  的整体质量*，  使其 *更易于理解、更健壮、更易于维护*。  虽然不能 *完全保证*  一定不会再遇到任何问题 (因为环境的复杂性)，  但我相信 *重构后的 Workflow  将 *更可靠*，  之前遇到的 YAML  语法错误问题应该 *不会再出现*，  `temp_mapping.csv`  文件找不到的问题，  在  `working-directory`  配置正确的情况下，  也 *应该能得到解决*。

请您 *尽快尝试*  并告知我 *这次运行的结果*。  我将 *持续关注*  并提供支持！  感谢您的信任和配合！
