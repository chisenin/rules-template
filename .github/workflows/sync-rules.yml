    - name: Process rules-list.txt and update mapping.csv with Python
      id: process_rules
      run: |
        python3 << 'EOF'
        import csv
        import os
        import requests
        import hashlib
        from urllib.parse import urlparse

        rules_list_path = "${{ env.RULES_LIST }}"
        mapping_file_path = "${{ env.MAPPING_FILE }}"
        temp_mapping_file_path = "${{ env.TEMP_MAPPING_FILE }}"
        del_list_file_path = "${{ env.DEL_LIST_FILE }}"
        output_dir = "${{ env.OUTPUT_DIR }}"

        def calculate_hash(file_path):
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()

        # Read existing mappings from CSV
        existing_mappings = {}
        if os.path.isfile(mapping_file_path):
            with open(mapping_file_path, mode='r', newline='') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    existing_mappings[row['URL']] = row

        # Read URLs from rules-list.txt
        new_urls = set()
        with open(rules_list_path, mode='r') as file:
            for line in file:
                url = line.strip()
                if url.startswith(('http://', 'https://')):
                    new_urls.add(url)

        # Create header if temp_mapping_file does not exist
        if not os.path.isfile(temp_mapping_file_path):
            with open(temp_mapping_file_path, mode='w', newline='') as file:
                writer = csv.writer(file)
                writer.writerow(['URL', 'Remote_Filename', 'Local_Filename', 'Hash'])

        # Compare and update temp_mapping.csv
        with open(temp_mapping_file_path, mode='a', newline='') as temp_file:
            writer = csv.writer(temp_file)
            filename_count = {}
            conflict_filenames = {}

            for url in new_urls:
                response = requests.head(url, allow_redirects=True)
                remote_filename = os.path.basename(urlparse(response.url).path)
                local_filename = remote_filename

                # Count occurrences of each remote_filename
                if remote_filename in filename_count:
                    filename_count[remote_filename] += 1
                else:
                    filename_count[remote_filename] = 1

                # Track filenames with conflicts
                if filename_count[remote_filename] > 1:
                    if remote_filename not in conflict_filenames:
                        conflict_filenames[remote_filename] = []
                    conflict_filenames[remote_filename].append(url)

                # Calculate hash for existing file if it exists
                local_file_path = os.path.join(output_dir, local_filename)
                existing_hash = None
                if os.path.isfile(local_file_path):
                    existing_hash = calculate_hash(local_file_path)

                # Fetch remote hash
                response = requests.get(url, stream=True)
                remote_hash = hashlib.md5(response.content).hexdigest()

                # Write to temp_mapping.csv if there's a change or no existing entry
                if url not in existing_mappings or existing_mappings[url]['Hash'] != remote_hash:
                    writer.writerow([url, remote_filename, local_filename, remote_hash])
                else:
                    writer.writerow(existing_mappings[url].values())

            # Handle conflicts by renaming files
            for remote_filename, urls in conflict_filenames.items():
                for idx, url in enumerate(urls):
                    response = requests.head(url, allow_redirects=True)
                    remote_filename = os.path.basename(urlparse(response.url).path)
                    parsed_url = urlparse(url)
                    last_segment = parsed_url.path.split('/')[-2]
                    local_filename = f"{last_segment}-{remote_filename}-{idx+1}" if idx > 0 else remote_filename

                    # Calculate hash for existing file if it exists
                    local_file_path = os.path.join(output_dir, local_filename)
                    existing_hash = None
                    if os.path.isfile(local_file_path):
                        existing_hash = calculate_hash(local_file_path)

                    # Fetch remote hash
                    response = requests.get(url, stream=True)
                    remote_hash = hashlib.md5(response.content).hexdigest()

                    # Update temp_mapping.csv with the new local_filename
                    writer.writerow([url, remote_filename, local_filename, remote_hash])

            # Identify files to delete
            del_list = []
            for url in existing_mappings:
                if url not in new_urls:
                    del_list.append(existing_mappings[url]['Local_Filename'])

            # Write del_list to file
            with open(del_list_file_path, mode='w') as del_file:
                for filename in del_list:
                    del_file.write(f"{filename}\n")
        EOF
