name: Sync Rules Files

on:
  push:
    paths:
      - rules-list.txt
  schedule:
    - cron: '0 4 * * *'
  workflow_dispatch:

env:
  RULES_LIST: rules-list.txt
  MAPPING_FILE: mapping.csv
  TEMP_MAPPING_FILE: temp_mapping.csv
  DEL_LIST_FILE: del_list.txt
  OUTPUT_DIR: rules

jobs:
  sync-rules:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install Python dependencies
      run: pip install requests

    - name: Create output directory if not exists
      run: mkdir -p ${{ env.OUTPUT_DIR }}

    - name: Process rules-list.txt and update mapping.csv with Python
      id: process_rules
      run: |
        python3 << 'EOF'
        import csv
        import os
        import requests
        import hashlib
        from urllib.parse import urlparse
        from datetime import datetime

        rules_list_path = "${{ env.RULES_LIST }}"
        mapping_file_path = "${{ env.MAPPING_FILE }}"
        temp_mapping_file_path = "${{ env.TEMP_MAPPING_FILE }}"
        del_list_file_path = "${{ env.DEL_LIST_FILE }}"
        output_dir = "${{ env.OUTPUT_DIR }}"

        def calculate_hash(file_path):
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()

        # 检查映射文件是否存在，不存在则创建并写入 CSV header
        if not os.path.isfile(mapping_file_path):
            with open(mapping_file_path, mode='w', newline='') as file:
                writer = csv.writer(file)
                writer.writerow(['URL', 'Remote_Filename', 'Local_Filename', 'Hash'])

        # 读取现有的映射关系
        existing_mappings = {}
        if os.path.isfile(mapping_file_path):
            with open(mapping_file_path, mode='r', newline='') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    existing_mappings[row['URL']] = row

        # 读取 rules-list.txt 中的 URL 列表
        new_mappings = {}
        with open(rules_list_path, mode='r') as file:
            for line in file:
                url = line.strip()
                if url.startswith(('http://', 'https://')):
                    response = requests.head(url, allow_redirects=True)
                    remote_filename = os.path.basename(urlparse(response.url).path)

                    # 计算远程文件哈希值
                    response = requests.get(url, stream=True)
                    remote_hash = hashlib.md5(response.content).hexdigest()

                    local_filename = remote_filename # 默认本地文件名与远程文件名相同

                    # 如果 URL 不存在于现有映射或哈希值不同，则添加到新映射
                    existing_hash = existing_mappings.get(url, {}).get('Hash')
                    if url not in existing_mappings or existing_hash != remote_hash:
                        new_mappings[url] = {
                            'Remote_Filename': remote_filename,
                            'Local_Filename': local_filename,
                            'Hash': remote_hash
                        }


        # 如果临时映射文件不存在，则创建并写入 CSV header (始终创建 temp_mapping.csv)
        with open(temp_mapping_file_path, mode='w', newline='') as temp_file: # 始终创建并写入 header
            writer = csv.writer(temp_file)
            writer.writerow(['URL', 'Remote_Filename', 'Local_Filename', 'Hash'])


        # 处理文件名冲突
        filename_count = {}
        conflict_remote_filenames = set()
        existing_local_filenames = set()
        for row in existing_mappings.values():
            existing_local_filenames.add(row['Local_Filename'])

        for url, info in new_mappings.items():
            remote_filename = info['Remote_Filename']
            local_filename = info['Local_Filename']

            # 统计 remote_filename 在 new_mappings 中的出现次数
            if remote_filename in filename_count:
                filename_count[remote_filename] += 1
            else:
                filename_count[remote_filename] = 1

            # 检测冲突:
            # 1. new_mappings 中 remote_filename 出现多次
            # 2. remote_filename 与 existing_mappings 中的 Local_Filename 重名
            if filename_count[remote_filename] > 1 or remote_filename in existing_local_filenames:
                conflict_remote_filenames.add(remote_filename) # 将冲突的 remote_filename 加入集合


        # 调整所有冲突的 remote_filename 的 Local_Filename
        updated_mappings = {}
        for url, row in existing_mappings.items():
            remote_filename = row['Remote_Filename']
            local_filename = row['Local_Filename']
            if remote_filename in conflict_remote_filenames: # 如果是冲突文件名，则生成新的本地文件名
                parsed_url = urlparse(url)
                last_segment = parsed_url.path.split('/')[-2]
                local_filename = f"{last_segment}-{remote_filename}"
            updated_mappings[url] = {'Remote_Filename': remote_filename, 'Local_Filename': local_filename, 'Hash': row['Hash']} # 保留 Hash 值

        for url, info in new_mappings.items():
            remote_filename = info['Remote_Filename']
            local_filename = info['Local_Filename']
            hash_value = info['Hash']
            if remote_filename in conflict_remote_filenames: # 如果是冲突文件名, 也生成新的本地文件名
                parsed_url = urlparse(url)
                last_segment = parsed_url.path.split('/')[-2]
                local_filename = f"{last_segment}-{remote_filename}"
            updated_mappings[url] = {'Remote_Filename': remote_filename, 'Local_Filename': local_filename, 'Hash': hash_value}


        # 更新临时映射文件 temp_mapping.csv (写入 updated_mappings)
        with open(temp_mapping_file_path, mode='w', newline='') as temp_file: # 再次以 'w' 模式打开，确保文件已创建
            writer = csv.writer(temp_file)
            writer.writerow(['URL', 'Remote_Filename', 'Local_Filename', 'Hash']) # 再次写入 header (虽然之前已经写入，但确保万无一失)
            for url, info in updated_mappings.items():
                writer.writerow([url, info['URL' if 'URL' in info else 'Remote_Filename'], info['Remote_Filename'], info['Local_Filename'], info['Hash']])


        # 找出需要删除的文件
        del_list = []
        updated_urls = set(updated_mappings.keys())
        for url, row in existing_mappings.items():
            if url not in updated_urls: # 如果旧 URL 不在新 URL 列表中，则需要删除
                del_list.append(row['Local_Filename'])

        # 将删除列表写入 del_list.txt
        with open(del_list_file_path, mode='w') as del_file:
            for filename in del_list:
                del_file.write(f"{filename}\n")

        # 更新 mapping.csv
        os.replace(temp_mapping_file_path, mapping_file_path)
        EOF

    - name: Download updated files based on temp_mapping.csv
      run: |
        if [ -f "${{ env.TEMP_MAPPING_FILE }}" ]; then
          while IFS=',' read -r url remote_filename local_filename hash; do
            local_file="${{ env.OUTPUT_DIR }}/${local_filename}"
            if ! test -f "$local_file" || [ "$(md5sum $local_file | awk '{ print $1 }')" != "$hash" ]; then
              wget --continue -O "$local_file" "$url" || echo "Failed to download $url. Skipping."
            fi
          done < <(tail -n +2 "${{ env.TEMP_MAPPING_FILE }}")
        else
          echo "No new or updated files to download."
        fi

    - name: Delete obsolete files based on del_list.txt
      run: |
        if [ -f "${{ env.DEL_LIST_FILE }}" ]; then
          while IFS= read -r filename; do
            rm -f "${{ env.OUTPUT_DIR }}/$filename" || echo "Failed to delete $filename. Skipping."
          done < "${{ env.DEL_LIST_FILE }}"
        else
          echo "No obsolete files to delete."
        fi

    - name: Update mapping.csv
      run: |
        cp "${{ env.TEMP_MAPPING_FILE }}" "${{ env.MAPPING_FILE }}"
        rm "${{ env.TEMP_MAPPING_FILE }}"
        rm "${{ env.DEL_LIST_FILE }}"

    - name: List files in rules directory
      run: ls -la "${{ env.OUTPUT_DIR }}"

    - name: Add changes to staging
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        git add "${{ env.OUTPUT_DIR }}"
        git add "${{ env.MAPPING_FILE }}"

    - name: Commit changes
      run: |
        git diff-index --quiet HEAD || git commit -m "Update rules files from rules-list.txt"

    - name: Push changes
      uses: ad-m/github-push-action@master
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
