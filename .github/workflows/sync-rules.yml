name: Sync Rules Files

on:
  push:
    paths:
      - rules-list.txt
  schedule:
    - cron: '0 4 * * *'
  workflow_dispatch:

env:
  RULES_LIST: rules-list.txt
  MAPPING_FILE: mapping.csv
  OUTPUT_DIR: rules

jobs:
  sync-rules:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install Python dependencies
      run: pip install requests

    - name: Create output directory if not exists
      run: mkdir -p ${{ env.OUTPUT_DIR }}

    - name: Validate rules-list.txt exists with valid URLs
      run: |
        if [ ! -f "${{ env.RULES_LIST }}" ]; then
          echo "Error: ${RULES_LIST} file does not exist."
          exit 1
        fi
        valid_rules=$(grep -E '^https?://' "${{ env.RULES_LIST }}" | wc -l)
        if [ "$valid_rules" -eq 0 ]; then
          echo "Error: ${RULES_LIST} contains no valid URLs."
          exit 1
        fi

    - name: Process rules-list.txt and update mapping.csv with Python
      id: process_rules
      run: |
        python3 << 'EOF'
        import csv
        import os
        import requests
        from datetime import datetime

        rules_list_path = "${{ env.RULES_LIST }}"
        mapping_file_path = "${{ env.MAPPING_FILE }}"
        output_dir = "${{ env.OUTPUT_DIR }}"

        # Check if mapping file exists and create header if not
        mappings = {
            'urls': {},
            'filename_count': {}
        }
        if os.path.isfile(mapping_file_path):
            with open(mapping_file_path, mode='r', newline='') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    mappings['urls'][row['URL']] = {
                        'Remote_Filename': row['Remote_Filename'],
                        'Local_Filename': row['Local_Filename'],
                        'Last_Modified': datetime.strptime(row['Last_Modified'], '%Y-%m-%d %H:%M:%S') if row['Last_Modified'] else None
                    }
                    # Count occurrences of each local filename
                    if row['Local_Filename'] in mappings['filename_count']:
                        mappings['filename_count'][row['Local_Filename']] += 1
                    else:
                        mappings['filename_count'][row['Local_Filename']] = 1
        else:
            print(f"Mapping file {mapping_file_path} does not exist, creating new one.")

        # Read URLs and last modified times from rules-list.txt
        for line in open(rules_list_path, mode='r'):
            url = line.strip()
            if url.startswith(('http://', 'https://')):
                response = requests.head(url, allow_redirects=True)
                remote_filename = os.path.basename(url)
                last_modified_str = response.headers.get('last-modified', None)
                if last_modified_str:
                    last_modified = datetime.strptime(last_modified_str, '%a, %d %b %Y %H:%M:%S %Z')
                else:
                    last_modified = None

                if url in mappings['urls']:
                    existing_last_modified = mappings['urls'][url]['Last_Modified']
                    if last_modified != existing_last_modified:
                        print(f"Update for URL: {url}, Remote Filename: {remote_filename}")
                        local_filename = mappings['urls'][url]['Local_Filename']
                        # Need to adjust local filenames if there are any issues
                        if mappings['filename_count'][local_filename] > 1:
                            url_segments = url.split('/')
                            last_segment = url_segments[-2]
                            local_filename = f"{last_segment}-{remote_filename}"
                            mappings['filename_count'][local_filename] = 1  # Mark as handled
                        mappings['urls'][url]['Local_Filename'] = local_filename
                        mappings['urls'][url]['Last_Modified'] = last_modified
                else:
                    print(f"New URL: {url}, Remote Filename: {remote_filename}")
                    local_filename = remote_filename
                    if local_filename in mappings['filename_count']:
                        url_segments = url.split('/')
                        last_segment = url_segments[-2]
                        local_filename = f"{last_segment}-{remote_filename}"
                        mappings['filename_count'][local_filename] = 1  # Mark as handled
                    else:
                        mappings['filename_count'][local_filename] = 1

                    mappings['urls'][url] = {
                        'Remote_Filename': remote_filename,
                        'Local_Filename': local_filename,
                        'Last_Modified': last_modified
                    }

        # Write the updated mappings to CSV
        with open(mapping_file_path, mode='w', newline='') as file:
            fieldnames = ['URL', 'Remote_Filename', 'Local_Filename', 'Last_Modified']
            writer = csv.DictWriter(file, fieldnames=fieldnames)
            writer.writeheader()
            for url, info in mappings['urls'].items():
                last_modified_str = info['Last_Modified'].strftime('%Y-%m-%d %H:%M:%S') if info['Last_Modified'] else None
                writer.writerow({
                    'URL': url,
                    'Remote_Filename': info['Remote_Filename'],
                    'Local_Filename': info['Local_Filename'],
                    'Last_Modified': last_modified_str
                })

        # Create a CSV with only new or updated mappings
        with open('updated_mappings.csv', mode='w', newline='') as file:
            fieldnames = ['URL', 'Remote_Filename', 'Local_Filename', 'Last_Modified']
            writer = csv.DictWriter(file, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows([v for k, v in mappings['urls'].items()])
        EOF

    - name: Download updated files based on mapping.csv
      run: |
        # Read URLs with updates from updated_mappings.csv
        if [ -f "updated_mappings.csv" ]; then
          while IFS=',' read -r url remote_filename local_filename last_modified; do
            local_file="${{ env.OUTPUT_DIR }}/${local_filename}"
            echo "Downloading $url as $local_filename"
            wget --continue -O "$local_file" "$url" || echo "Failed to download $url. Skipping."
          done < <(tail -n +2 "updated_mappings.csv")
        else
          echo "No new or updated files to download."
        fi

    - name: List files in rules directory
      run: ls -la "${{ env.OUTPUT_DIR }}"

    - name: Add changes to staging
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        git add "${{ env.OUTPUT_DIR }}"
        git add "${{ env.MAPPING_FILE }}"

    - name: Commit changes
      run: |
        git diff-index --quiet HEAD || git commit -m "Update rules files from rules-list.txt"

    - name: Push changes
      uses: ad-m/github-push-action@master
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
