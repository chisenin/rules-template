name: Sync Rules Files

on:
  push:
    paths:
      - rules-list.txt
  schedule:
    - cron: '0 4 * * *'
  workflow_dispatch:

env:
  RULES_LIST: rules-list.txt
  MAPPING_FILE: mapping.csv
  TEMP_MAPPING_FILE: temp_mapping.csv
  DEL_LIST_FILE: del_list.txt
  OUTPUT_DIR: rules

jobs:
  sync-rules:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install Python dependencies
      run: pip install requests

    - name: Create output directory if not exists
      run: mkdir -p ${{ env.OUTPUT_DIR }}

    - name: Process rules-list.txt and update mapping.csv with Python
      id: process_rules
      run: |
        python3 << 'EOF'
        import csv
        import os
        import requests
        import hashlib
        from urllib.parse import urlparse
        from datetime import datetime

        rules_list_path = "${{ env.RULES_LIST }}" # 规则列表文件路径
        mapping_file_path = "${{ env.MAPPING_FILE }}" # 映射文件路径
        temp_mapping_file_path = "${{ env.TEMP_MAPPING_FILE }}" # 临时映射文件路径
        del_list_file_path = "${{ env.DEL_LIST_FILE }}" # 删除列表文件路径
        output_dir = "${{ env.OUTPUT_DIR }}" # 输出目录

        def calculate_hash(file_path):
            # 计算文件 MD5 哈希值
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()

        # 检查映射文件是否存在，不存在则创建并写入 CSV header
        if not os.path.isfile(mapping_file_path):
            with open(mapping_file_path, mode='w', newline='') as file:
                writer = csv.writer(file)
                writer.writerow(['URL', 'Remote_Filename', 'Local_Filename', 'Hash'])

        # 读取现有的映射关系
        existing_mappings = {}
        if os.path.isfile(mapping_file_path):
            with open(mapping_file_path, mode='r', newline='') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    existing_mappings[row['URL']] = row

        # 读取 rules-list.txt 中的 URL 列表
        new_mappings = {}
        with open(rules_list_path, mode='r') as file:
            for line in file:
                url = line.strip()
                if url.startswith(('http://', 'https://')):
                    response = requests.head(url, allow_redirects=True) # 获取重定向后的 URL
                    remote_filename = os.path.basename(urlparse(response.url).path) # 从 URL 中提取文件名

                    # 计算远程文件哈希值
                    response = requests.get(url, stream=True)
                    remote_hash = hashlib.md5(response.content).hexdigest()

                    local_filename = remote_filename # 默认本地文件名与远程文件名相同

                    # 如果 URL 不存在于现有映射或哈希值不同，则添加到新映射
                    existing_hash = existing_mappings.get(url, {}).get('Hash')
                    if url not in existing_mappings or existing_hash != remote_hash:
                        new_mappings[url] = {
                            'Remote_Filename': remote_filename,
                            'Local_Filename': local_filename,
                            'Hash': remote_hash
                        }

        # 处理文件名冲突
        filename_count = {} # 统计远程文件名出现次数
        conflict_filenames = {} # 存储冲突文件名及其对应的 URLs

        for url, info in new_mappings.items():
            remote_filename = info['Remote_Filename']
            local_filename = info['Local_Filename']

            if remote_filename in filename_count:
                filename_count[remote_filename] += 1
            else:
                filename_count[remote_filename] = 1

            if filename_count[remote_filename] > 1: # 发现文件名冲突
                if remote_filename not in conflict_filenames:
                    conflict_filenames[remote_filename] = []
                conflict_filenames[remote_filename].append((url, remote_filename))

        # 调整本地文件名以解决冲突
        for remote_filename, entries in conflict_filenames.items():
            for url, _ in entries: # 遍历冲突文件名对应的所有 URL 条目
                parsed_url = urlparse(url)
                last_segment = parsed_url.path.split('/')[-2] # 获取 URL 倒数第二段作为本地文件名前缀
                local_filename = f"{last_segment}-{remote_filename}" # 生成新的本地文件名
                new_mappings[url]['Local_Filename'] = local_filename # 更新本地文件名

        # 如果临时映射文件不存在，则创建并写入 CSV header
        if not os.path.isfile(temp_mapping_file_path):
            with open(temp_mapping_file_path, mode='w', newline='') as file:
                writer = csv.writer(file)
                writer.writerow(['URL', 'Remote_Filename', 'Local_Filename', 'Hash'])

        # 更新临时映射文件 temp_mapping.csv
        with open(temp_mapping_file_path, mode='a', newline='') as temp_file:
            writer = csv.writer(temp_file)
            for url, info in new_mappings.items():
                remote_filename = info['Remote_Filename']
                local_filename = info['Local_Filename']
                hash_value = info['Hash']
                writer.writerow([url, remote_filename, local_filename, hash_value]) # 写入 URL, 远程文件名, 本地文件名和哈希值到 CSV

        # 找出需要删除的文件
        del_list = []
        for url in existing_mappings:
            if url not in new_mappings:
                del_list.append(existing_mappings[url]['Local_Filename']) # 将需要删除的本地文件名添加到 del_list

        # 将删除列表写入 del_list.txt
        with open(del_list_file_path, mode='w') as del_file:
            for filename in del_list:
                del_file.write(f"{filename}\n")
        EOF

    - name: Download updated files based on temp_mapping.csv
      run: |
        if [ -f "${{ env.TEMP_MAPPING_FILE }}" ]; then
          while IFS=',' read -r url remote_filename local_filename hash; do
            local_file="${{ env.OUTPUT_DIR }}/${local_filename}"
            if ! test -f "$local_file" || [ "$(md5sum $local_file | awk '{ print $1 }')" != "$hash" ]; then
              wget --continue -O "$local_file" "$url" || echo "Failed to download $url. Skipping."
            fi
          done < <(tail -n +2 "${{ env.TEMP_MAPPING_FILE }}")
        else
          echo "No new or updated files to download."
        fi

    - name: Delete obsolete files based on del_list.txt
      run: |
        if [ -f "${{ env.DEL_LIST_FILE }}" ]; then
          while IFS= read -r filename; do
            rm -f "${{ env.OUTPUT_DIR }}/$filename" || echo "Failed to delete $filename. Skipping."
          done < "${{ env.DEL_LIST_FILE }}"
        else
          echo "No obsolete files to delete."
        fi

    - name: Update mapping.csv
      run: |
        cp "${{ env.TEMP_MAPPING_FILE }}" "${{ env.MAPPING_FILE }}"
        rm "${{ env.TEMP_MAPPING_FILE }}"
        rm "${{ env.DEL_LIST_FILE }}"

    - name: List files in rules directory
      run: ls -la "${{ env.OUTPUT_DIR }}"

    - name: Add changes to staging
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        git add "${{ env.OUTPUT_DIR }}"
        git add "${{ env.MAPPING_FILE }}"

    - name: Commit changes
      run: |
        git diff-index --quiet HEAD || git commit -m "Update rules files from rules-list.txt"

    - name: Push changes
      uses: ad-m/github-push-action@master
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
